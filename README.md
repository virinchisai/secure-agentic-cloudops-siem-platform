# ğŸ” Secure Agentic CloudOps SIEM Platform

A cloud-native, event-driven **SIEM-style security and CloudOps platform** that demonstrates **log ingestion, streaming pipelines, detection engineering, and scalable backend system design**.

This project simulates how modern security platforms ingest logs, process them in real time, detect suspicious activity, and persist alerts for investigation â€” while also outlining a clear roadmap toward **agentic, LLM-powered security automation**.

---

## ğŸ“› Badges

<p align="left">
  <img src="https://img.shields.io/github/languages/top/virinchisai/secure-agentic-cloudops-siem-platform" />
  <img src="https://img.shields.io/github/repo-size/virinchisai/secure-agentic-cloudops-siem-platform" />
  <img src="https://img.shields.io/badge/Code%20Style-Black-000000" />
</p>

<!-- **Note:** CI/CD, Tests, SAST, secrets scanning, and docs badges will be added once implemented. -->

---

## ğŸ¯ Project Objectives

- **Ingest logs:** Accept raw security logs through a REST API.
- **Stream events:** Stream logs using Kafka-compatible infrastructure (Redpanda).
- **Detect in real time:** Apply detection logic and scoring as events flow through the system.
- **Persist signals:** Store normalized events and alerts in PostgreSQL for investigation.
- **Support investigation:** Enable query-driven validation and basic investigation workflows.
- **Show system design:** Demonstrate production-grade, decoupled microservice architecture.
- **Enable a roadmap:** Provide a clear path to agentic security automation.

---

## ğŸš§ Implementation Status

This repository intentionally includes **both implemented components and a forward-looking architecture**.

### âœ… Implemented (fully working)

- **Ingest service:** FastAPI-based log ingestion.
- **Streaming:** Kafka-compatible event streaming using Redpanda.
- **Detection:** Detection service consuming Kafka events.
- **Persistence:** PostgreSQL persistence for events and alerts.
- **Validation:** End-to-end data flow validation.
- **One-command run:** Local execution via `scripts/run_all.sh`.
- **Infra:** Dockerized infrastructure.

### ğŸ§­ Planned / roadmap

- **Agentic reasoning:** Agentic LLM reasoning engine.
- **Knowledge retrieval:** Vector knowledge base for contextual retrieval.
- **Automation:** Automated remediation workflows.
- **Gateway & auth:** API gateway with authentication (JWT/OAuth/RBAC).
- **UI:** SIEM dashboards and visualizations.
- **Cloud:** Cloud deployment (AWS / GCP).
- **Engineering:** CI/CD pipelines (tests, SAST, secrets scanning).

> âš ï¸ **Scope note**  
> Advanced components described later in this README (agentic reasoning, vector retrieval, workflows, API gateway, cloud deployment)
> represent the **architectural roadmap and system design intent**.  
> The current implementation focuses on a production-grade ingestion, streaming, detection, and persistence pipeline.

---

## ğŸ§­ Table of contents

- [High-level architecture current implementation](#-high-level-architecture-current-implementation)
- [Architecture explanation step-by-step](#-architecture-explanation-step-by-step)
- [Future-state architecture roadmap](#-future-state-architecture-roadmap)
- [Technology stack](#-technology-stack)
- [Ports and services](#-ports-and-services)
- [Repository structure](#-repository-structure)
- [Quick start one command](#-quick-start-one-command)
- [Verify after running](#-verify-after-running)
- [Stop and reset](#-stop-and-reset)
- [Service breakdown](#-service-breakdown)
- [Developer experience local development](#-developer-experience-local-development)
- [CI/CD workflows planned](#-cicd-workflows-planned)
- [Deployment current and planned](#-deployment-current-and-planned)
- [Production hardening planned](#-production-hardening-planned)
- [Observability planned](#-observability-planned)
- [Future enhancements](#-future-enhancements)
- [License](#-license)

---

## ğŸ—ï¸ High-level architecture current implementation

```text
Client (curl / scripts / agents)
        |
        v
Ingest Service (FastAPI)
        |
        v
Kafka / Redpanda (topic: logs.raw)
        |
        v
Detection Service (Kafka Consumer)
        |
        v
PostgreSQL
   â”œâ”€â”€ events   (normalized logs)
   â””â”€â”€ alerts   (scored detections)
```

---

## ğŸ§  Architecture explanation step-by-step

### 1ï¸âƒ£ Client

- **Represents:** VPN gateways, authentication servers, firewalls, cloud services, apps.
- **Responsibility:** Send logs as JSON payloads over HTTP.

### 2ï¸âƒ£ Ingest service (FastAPI)

- **Responsibilities:**
  - **Validate:** Validate incoming payload structure.
  - **Normalize:** Normalize logs into a consistent event model.
  - **Identify:** Assign a unique `event_id`.
  - **Publish:** Publish normalized events to Kafka topic `logs.raw`.
- **Why it matters:**
  - **Decoupling:** Producers are decoupled from consumers.
  - **Scale:** Ingest can scale independently.
  - **Resilience:** Kafka buffers spikes and protects downstream services.

### 3ï¸âƒ£ Kafka / Redpanda

- **Responsibilities:**
  - **Backbone:** Event streaming backbone.
  - **Buffering:** Reliable buffering and replay.
  - **Distribution:** Fan-out to one or more consumers.

### 4ï¸âƒ£ Detection service

- **Responsibilities:**
  - **Consume:** Consume from `logs.raw`.
  - **Score:** Apply detection rules / scoring.
  - **Persist:** Write alerts to PostgreSQL.
  - **Emit:** Optionally emit downstream topics (example: `alerts.scored`).

### 5ï¸âƒ£ PostgreSQL

- **Responsibilities:**
  - **Store:** Store normalized events + derived alerts.
  - **Investigate:** Support investigation queries and historical analysis.
- **Tables:**
  - **events:** Normalized raw logs.
  - **alerts:** Scored detections / security signals.

## ğŸ§­ Future-state architecture roadmap

This section is the **target architecture** for agentic, LLM-powered evolution.

### High-level system architecture vision

```mermaid
flowchart LR
  subgraph Client
    UI(Web UI / CLI)
  end

  subgraph API["API Layer"]
    GW(API Gateway)
    BE(FastAPI Backend)
  end

  subgraph Data["Data Layer"]
    VDB[(Vector DB)]
    DB[(PostgreSQL)]
    MQ[(Message Queue)]
    OBJ[(Object Storage)]
  end

  subgraph Processing["Processing Layer"]
    ING(Ingestion Pipeline)
    DET(Detection Engine)
    AG(Agentic Reasoning Engine)
    WF(Workflow Orchestrator)
  end

  UI --> GW --> BE
  BE --> ING
  BE --> DET
  BE --> AG
  BE --> WF

  ING --> MQ
  DET --> MQ
  AG --> MQ
  WF --> MQ

  MQ --> BE
  BE --> VDB
  BE --> DB
  BE --> OBJ
```
### Data flow overview vision
```mermaid
sequenceDiagram
  participant Src as Log Sources
  participant ING as Ingestion Pipeline
  participant MQ as Message Queue
  participant DET as Detection Engine
  participant AG as Agentic Engine
  participant WF as Workflow Engine
  participant API as API Backend

  Src->>ING: Send logs / alerts / telemetry
  ING->>MQ: Normalize + publish events
  MQ->>DET: Trigger detection rules
  DET->>MQ: Emit detection results
  MQ->>AG: Request reasoning + enrichment
  AG->>MQ: Emit triage decisions
  MQ->>WF: Trigger remediation workflow
  WF->>API: Update status / artifacts
```
## ğŸ” Technology stack

### Backend and streaming

- **Language:** Python 3.12
- **API framework:** FastAPI
- **Streaming:** Redpanda (Kafka-compatible)
- **Database:** PostgreSQL

### Tooling and infrastructure

- **Containers:** Docker, Docker Compose
- **Python env:** Poetry
- **Dev:** VS Code (recommended)

## ğŸ”Œ Ports and services

| Service | Description | Port | Access |
|---|---|---:|---|
| Ingest Service | Log ingestion API | 8001 | `http://127.0.0.1:8001` |
| Detection Service | Detection + Kafka consumer API | 8002 | `http://127.0.0.1:8002` |
| Redpanda (Kafka) | Streaming broker | 9092 | PLAINTEXT |
| Redpanda HTTP API | Broker admin | 8082 | `http://127.0.0.1:8082` |
| Redpanda Console | Kafka UI | 8080 | `http://127.0.0.1:8080` |
| PostgreSQL | Events & alerts DB | 5432 | `cloudops` database |
| MLflow | Experiment tracking | 5001 | `http://127.0.0.1:5001` |

## ğŸ“‚ Repository structure

```text
secure-agentic-cloudops-siem-platform/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ingest-service/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”‚   â””â”€â”€ pyproject.toml
â”‚   â””â”€â”€ detection-service/
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â””â”€â”€ main.py
â”‚       â””â”€â”€ pyproject.toml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_all.sh
â”‚   â”œâ”€â”€ stop_all.sh
â”‚   â”œâ”€â”€ reset_all.sh
â”‚   â””â”€â”€ seed_sample_events.py
â””â”€â”€ docs/

```

## ğŸ§© Service Breakdown

Each major component is modular and independently deployable.  
Click to expand each section for details.

---

<details>
<summary><strong>ğŸ§© Ingestion Pipeline</strong></summary>

**Purpose:**  
Handles log, alert, and telemetry ingestion from multiple sources.

**Modules:**  
- `parsers/` â€” Sourceâ€‘specific parsers  
- `normalizers/` â€” Schema normalization  
- `enrichers/` â€” Metadata enrichment  
- `publishers/` â€” Event dispatch to message queue  

**Supported Sources:**  
- Cloud provider logs  
- Security alerts  
- Application telemetry  
- Custom connectors  

**Output:**  
Normalized events published to `Message Queue`

</details>

---

<details>
<summary><strong>ğŸ§  Agentic Reasoning Engine</strong></summary>

**Purpose:**  
Uses LLMs to reason over events, enrich context, and generate triage decisions.

**Modules:**  
- `context_builder/` â€” Builds event context  
- `retriever/` â€” Vector DB retrieval  
- `llm_core/` â€” LLM reasoning logic  
- `policy_engine/` â€” Decision policies  
- `action_generator/` â€” Suggested actions  

**LLM Providers:**  
- OpenAI  
- Azure OpenAI  
- Anthropic  
- Local models (via Ollama)  

**Output:**  
Triage decisions and suggested actions published to `Message Queue`

</details>

---

<details>
<summary><strong>ğŸ” Detection Engine</strong></summary>

**Purpose:**  
Applies detection rules and correlation logic to incoming events.

**Modules:**  
- `rules/` â€” YAMLâ€‘based detection rules  
- `correlators/` â€” Multiâ€‘event correlation  
- `matchers/` â€” Signature and anomaly matching  
- `emitters/` â€” Detection result publishing  

**Rule Packs:**  
- MITRE ATT&CK  
- Cloud misconfigurations  
- Custom rules  

**Output:**  
Detection results published to `Message Queue`

</details>

---

<details>
<summary><strong>ğŸ“š Vector Knowledge Layer</strong></summary>

**Purpose:**  
Stores embeddings of documentation, runbooks, and prior incidents for retrieval.

**Modules:**  
- `indexers/` â€” Embedding + ingestion  
- `retrievers/` â€” Queryâ€‘time retrieval  
- `chunkers/` â€” Text chunking strategies  
- `embedders/` â€” OpenAI / HuggingFace / Cohere  

**Sources:**  
- Markdown docs  
- PDFs  
- Incident timelines  
- External APIs  

**Output:**  
Contextual data for agentic reasoning

</details>

---

<details>
<summary><strong>âš™ï¸ Workflow Orchestrator</strong></summary>

**Purpose:**  
Executes multiâ€‘step remediation workflows triggered by agentic decisions.

**Modules:**  
- `triggers/` â€” Eventâ€‘based triggers  
- `steps/` â€” Declarative step definitions  
- `executors/` â€” Cloud API calls  
- `notifiers/` â€” Slack / Email / PagerDuty  

**Supported Actions:**  
- Quarantine instance  
- Rotate credentials  
- Block IP  
- Notify team  

**Output:**  
Remediation status updates

</details>

---

<details>
<summary><strong>ğŸŒ API Gateway & Backend</strong></summary>

**Purpose:**  
Exposes REST API for orchestration, status, and integrations.

**Modules:**  
- `routes/` â€” FastAPI endpoints  
- `services/` â€” Business logic  
- `auth/` â€” JWT / OAuth2  
- `docs/` â€” OpenAPI autoâ€‘docs  

**Endpoints:**  
- `/ingest`  
- `/detect`  
- `/reason`  
- `/workflow`  
- `/status`  
- `/health`  

**Output:**  
Unified API surface for all services

</details>
