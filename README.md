# ğŸ” Secure Agentic CloudOps SIEM Platform

![Python CI](https://github.com/virinchisai/secure-agentic-cloudops-siem-platform/actions/workflows/python-app.yml/badge.svg)

A cloud-native, event-driven **SIEM-style security and CloudOps platform** that demonstrates **log ingestion, streaming pipelines, detection engineering, and scalable backend system design**.

This project simulates how modern security platforms ingest logs, process them in real time, detect suspicious activity, and persist alerts for investigation â€” while also outlining a clear roadmap toward **agentic, LLM-powered security automation**.

---

## ğŸ“› Badges

<p align="left">
  <img src="https://img.shields.io/github/languages/top/virinchisai/secure-agentic-cloudops-siem-platform" />
  <img src="https://img.shields.io/github/repo-size/virinchisai/secure-agentic-cloudops-siem-platform" />
  <img src="https://img.shields.io/badge/Code%20Style-Black-000000" />
</p>

> Badges reflect repository metadata only.  
> CI/CD, tests, SAST, secrets scanning, and docs badges will be enabled once workflows are implemented.

---

## ğŸ¯ Project Objectives

- **Ingest logs:** Accept raw security logs through a REST API.
- **Stream events:** Stream logs using Kafka-compatible infrastructure (Redpanda).
- **Detect in real time:** Apply detection logic and scoring as events flow through the system.
- **Persist signals:** Store normalized events and alerts in PostgreSQL for investigation.
- **Support investigation:** Enable query-driven validation and basic investigation workflows.
- **Show system design:** Demonstrate production-grade, decoupled microservice architecture.
- **Enable a roadmap:** Provide a clear path to agentic security automation.

---

## ğŸš§ Implementation Status

This repository intentionally includes **both implemented components and a forward-looking architecture**.

### âœ… Implemented (fully working)

- Ingest service (FastAPI)
- Kafka-compatible streaming (Redpanda)
- Detection service consuming Kafka events
- PostgreSQL persistence for events and alerts
- End-to-end data flow validation
- One-command local execution (`scripts/run_all.sh`)
- Dockerized infrastructure

### ğŸ§­ Planned / roadmap

- Agentic LLM reasoning engine
- Vector knowledge base for contextual retrieval
- Automated remediation workflows
- API gateway with authentication (JWT / OAuth / RBAC)
- SIEM dashboards and visualizations
- Cloud deployment (AWS / GCP)
- CI/CD pipelines (tests, SAST, secrets scanning)

> âš ï¸ **Scope note**  
> Advanced components described later in this README represent **architectural intent and future roadmap**.  
> The current implementation focuses on a production-grade ingestion, streaming, detection, and persistence pipeline.

---

## ğŸ§­ Table of Contents

- High-level architecture (current)
- Architecture explanation (step-by-step)
- Future-state architecture roadmap
- Technology stack
- Ports and services
- Repository structure
- Quick start (one command)
- Verify after running
- Stop and reset
- Service breakdown
- Developer experience (local development)
- CI/CD workflows (planned)
- Deployment (current and planned)
- Production hardening (planned)
- Observability (planned)
- Future enhancements
- License

---

## ğŸ—ï¸ High-level architecture (current implementation)

```text
Client (curl / scripts / agents)
        |
        v
Ingest Service (FastAPI)
        |
        v
Kafka / Redpanda (topic: logs.raw)
        |
        v
Detection Service (Kafka Consumer)
        |
        v
PostgreSQL
   â”œâ”€â”€ events   (normalized logs)
   â””â”€â”€ alerts   (scored detections)
```

---

## ğŸ§  Architecture explanation (step-by-step)

### 1ï¸âƒ£ Client
- Represents VPN gateways, authentication servers, firewalls, cloud services, applications
- Sends logs as JSON payloads over HTTP

### 2ï¸âƒ£ Ingest service (FastAPI)
- Validates incoming payloads
- Normalizes logs into a consistent event model
- Assigns a unique `event_id`
- Publishes events to Kafka topic `logs.raw`

**Why it matters**
- Decouples producers from consumers
- Enables independent scaling
- Kafka buffers spikes and protects downstream services

### 3ï¸âƒ£ Kafka / Redpanda
- Event streaming backbone
- Reliable buffering and replay
- Fan-out to one or more consumers

### 4ï¸âƒ£ Detection service
- Consumes from `logs.raw`
- Applies detection rules and scoring
- Writes alerts to PostgreSQL
- Optionally emits derived topics (`alerts.scored`)

### 5ï¸âƒ£ PostgreSQL
- Stores normalized events and derived alerts
- Enables investigation and historical analysis

---

## ğŸ§­ Future-state architecture roadmap (vision)

```mermaid
flowchart LR
  subgraph Client
    UI(Web UI / CLI)
  end
  subgraph API["API Layer"]
    GW(API Gateway)
    BE(FastAPI Backend)
  end
  subgraph Data["Data Layer"]
    VDB[(Vector DB)]
    DB[(PostgreSQL)]
    MQ[(Message Queue)]
    OBJ[(Object Storage)]
  end
  subgraph Processing["Processing Layer"]
    ING(Ingestion)
    DET(Detection)
    AG(Agentic Reasoning)
    WF(Workflow Orchestrator)
  end
  UI --> GW --> BE
  BE --> ING --> MQ
  MQ --> DET --> MQ
  MQ --> AG --> MQ
  MQ --> WF --> API
```

---

## ğŸ” Technology stack

### Backend & streaming
- Python 3.12
- FastAPI
- Redpanda (Kafka-compatible)
- PostgreSQL

### Tooling & infrastructure
- Docker & Docker Compose
- Poetry
- VS Code

---

## ğŸ”Œ Ports and services

| Service | Port | Purpose |
|------|----:|------|
| Ingest Service | 8001 | Log ingestion API |
| Detection Service | 8002 | Detection + Kafka consumer |
| Redpanda (Kafka) | 9092 | Streaming broker |
| Redpanda Console | 8080 | Kafka UI |
| Redpanda HTTP API | 8082 | Broker admin |
| PostgreSQL | 5432 | Events & alerts DB |
| MLflow | 5001 | Experiment tracking |

---

## ğŸ“‚ Repository structure

```text
secure-agentic-cloudops-siem-platform/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ingest-service/
â”‚   â””â”€â”€ detection-service/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_all.sh
â”‚   â”œâ”€â”€ stop_all.sh
â”‚   â”œâ”€â”€ reset_all.sh
â”‚   â””â”€â”€ seed_sample_events.py
â””â”€â”€ docs/
```

---

## âš¡ Quick start (one command)

```bash
bash scripts/run_all.sh
```

This will:
- Start Docker infrastructure
- Create database tables
- Start ingest and detection services
- Send sample events
- Verify Kafka and PostgreSQL

---

## ğŸ” Verify after running

```bash
curl http://127.0.0.1:8001/health
curl http://127.0.0.1:8002/health
```

```bash
docker exec -it secure-agentic-cloudops-siem-platform-postgres-1 \
psql -U app -d cloudops -c "SELECT COUNT(*) FROM alerts;"
```

---

## ğŸ›‘ Stop and reset

```bash
bash scripts/stop_all.sh
```

```bash
bash scripts/reset_all.sh
```

---

## ğŸ§© Service breakdown

> Sections below marked **(Planned)** describe future roadmap components.

<details>
<summary><strong>ğŸ§  Agentic Reasoning Engine (Planned)</strong></summary>
LLM-powered reasoning, enrichment, and triage.
</details>

<details>
<summary><strong>âš™ï¸ Workflow Orchestrator (Planned)</strong></summary>
Automated remediation workflows.
</details>

---

## ğŸ§‘â€ğŸ’» Developer experience (local development)

- Python virtual environments managed via Poetry
- Services run independently for local iteration
- Hot reload enabled via `uvicorn --reload`
- Docker used only for shared infrastructure

---

## ğŸ”„ CI/CD workflows (planned)

- GitHub Actions
- Linting (Black, Ruff)
- Unit tests (pytest)
- Security scanning (SAST, secrets)
- Docker image builds

---

## ğŸš€ Deployment (current and planned)

### Current
- Local Docker Compose deployment

### Planned
- AWS (ECS / EKS)
- GCP (GKE)
- Terraform-based IaC

---

## ğŸ” Production hardening (planned)

- TLS everywhere
- Secrets management
- Kafka partitioning
- Database indexing
- Rate limiting
- RBAC

---

## ğŸ“Š Observability (planned)

- Structured logging
- Prometheus metrics
- Grafana dashboards
- OpenTelemetry tracing

---

## ğŸš§ Future enhancements

- Agentic remediation
- Cross-event correlation
- Threat intelligence enrichment
- Multi-tenant support

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
